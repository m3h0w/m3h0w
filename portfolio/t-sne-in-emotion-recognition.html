<!DOCTYPE html>
<html lang="en" class="gr__blackrockdigital_github_io"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <link href="../bootstrap.min.css" rel="stylesheet">

    <link href="./item_files/portfolio-item.css" rel="stylesheet">

    <link href="../style.css" rel="stylesheet">

    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">

  </head>

  <body data-gr-c-s-loaded="true">

    <!-- Navigation -->
    <div class="navbar-wrapper">
            <nav class="navbar navbar-expand-lg navbar-dark fixed-top">
                <div class="container">
                    <a class="navbar-brand" href="../portfolio.html">MICHAŁ GACKA</a>
                    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive"
                        aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <div class="collapse navbar-collapse" id="navbarResponsive">
                        <ul class="navbar-nav ml-auto">
                            <li class="nav-item">
                                <a class="nav-link" href="../portfolio.html">Portfolio
                                </a>
                            </li>
                            <li class="nav-item">
                                <a class="nav-link" href="../me.html">Me
                                </a>
                            </li>
                            </li>
                        </ul>
                    </div>
                </div>
            </nav>
            </div>

    <!-- Page Content -->
    <div class="container">

      <!-- Portfolio Item Heading -->


      <!-- Portfolio Item Row -->
      <div class="row">
            <div class="col-md-6 fake"></div>

        <div class="col-md-5 fixed">
                <h1 class="my-2">
                        Parametric t-SNE for temporal, multimodal emotion recognition
                        <small>
                            <!-- <a href='https://github.com/m3h0w/lstm-for-protein-tertiary-structure-prediction'>
                            <i class="fab fa-github"></i>
                            </a> -->
                            <a href='https://drive.google.com/file/d/1j5ZXntRgbDE_mr_5zZtdQdE6zzUx7Mts/view?usp=sharing'>
                                <i class="far fa-file-alt"></i>
                                </a>
                        </small>
                      </h1>
                      <p>University of Copenhagen, group work<br>
                        </p>
          <img class="img-fluid" src="../images/emotion_recognition.png" alt="">
        </div>

        <div class="col-md-6 project-text-column">
          <h2 class="my-2">Project description</h3>
          <p>The research aimed at improving understanding of multimodal emotion recognition models by introducing a framework for visualizing non-temporal and temporal outputs of models predicting multiple related values for each data entry.</p>
          <h4 class="my-3">Overview</h4>
          Various emotion recognition models are being introduced with the rise of powerful Machine Learning, computational power, and data sources. Most of them rely on Paul Ekman’s concept of 6 basic emotions, thus predict 6 values in parallel to describe single emotional state. 
          <br><br>This, together with data being merged from different modalities, makes interpreting the results a challenge and opens up opportunities for introducing better visualization techniques that could enable deeper insights, better understanding of the inner workings of machine learning, and ultimately better models.
        <br><br>Our work introduces a concept called emotion space, and overlays temporal information over that space, allowing for comparison between models and modality-dependent data sources. We present our results for training KNN and LSTM models on the <a href='https://github.com/A2Zadeh/CMU-MultimodalSDK'>CMU MOSEI dataset</a> and a dataset exploring emotion in music.
        <br><br>
        The course was built for a Cognitive Science course.
        <h4 class="my-3">Technologies</h4>
        Python, Parametric t-SNE, Keras, Sklearn, Plotly
        <h4 class="my-3">Technical Details</h4>
        The emotion space is created using Parametric t-SNE by embedding the 6-dimensional emotional state into a 2-dimensional space. It represents relationships between the emotions as understand through the prism of training data, grouping similar emotions together and e.g. placing sadness and happiness on one axis where high sadness and high happiness never coincide.
        <img class="my-2 img-fluid" src="../images/t-SNE/emotion_space.PNG" alt="">
        <p class='label text-center'>Emotion Space</p>
        The crucial point is the use of Parametric t-SNE instead of classical t-SNE. Given that we’ve trained a network that maps training data successfully from 6-, to 2-dimensional space, we can then map any unseen data point into that space again later, visualizing for example predictions of a model trying to learn multimodal emotion recognition task.
        <img class="my-2 img-fluid" src="../images/t-SNE/visualization_example.png" alt="">
        <p class='label text-center'>LSTM minimizing MSE by distinguishing only between happy and not-happy</p>
        An example could be an LSTM model that produced better MAE than KNN, yet after visualizing the results we could see that it only learned to differentiate between happy and sad.
        <br><br>For more information, the full report is available <a href='https://drive.google.com/file/d/1j5ZXntRgbDE_mr_5zZtdQdE6zzUx7Mts/view?usp=sharing'>HERE</a>
        <h4 class="my-3">My contribution</h4>
        I authored the idea, worked with parametric t-SNE to create the emotion space, and trained the LSTM model on multiple modalities.    
    </div>

      </div>
      <!-- /.row -->

    </div>
    <!-- /.container -->

    <!-- Footer -->
    <div class="spacer"></div>
    <a href='../portfolio.html'>
    <footer class="py-2 fixed footer">
      <div class="container">
        <p class="m-0 text-right text-black"><i class="fas fa-angle-double-left fa-2x"></i></p>
      </div>
    </footer>
    </a>

    <!-- Bootstrap core JavaScript -->
    <script src="./item_files/jquery.min.js.download"></script>
    <script src="./item_files/bootstrap.bundle.min.js.download"></script>

    <script>
        (function($) {
 
        jQuery( document ).ready(function() {
            add_target_blank_to_external_links();
        });

        function add_target_blank_to_external_links(){
            $('a[href^="http://"], a[href^="https://"]').not('a[href*="'+location.hostname+'"]').attr('target','_blank');
        }
        
        })(jQuery);
    </script>
</body></html>